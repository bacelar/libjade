
// the following implementation requires:
// - (even) param int CHACHA_ROUNDS;
// - inline fn __init_ref(reg u64 nonce key) -> stack u32[16] (check chacha_state.jinc)
// - inline fn __increment_counter_ref(stack u32[16] state) -> stack u32[16] (check chacha_state.jinc)

// used;
#nomodmsf inline fn __copy_state_ref(
  stack u32[16] st)
  ->
  reg u32[16],
  stack u32,
  stack u32
{
  inline int i;
  reg u32 k14 k15;
  reg u32[16] k;
  stack u32 s_k14 s_k15;

  k14 = st[14];
  s_k14 = k14;

  k15 = st[15];
  s_k15 = k15;

  for i=0 to 14
  { k[i] = st[i]; }

  return k, s_k14, s_k15;
}


///////////////////////////////////////////////////////////////////////////////

/*
// not exported; may be useful as spec;
inline fn __line_ref(reg u32[16] k, inline int a b c r) -> reg u32[16]
{
  k[a] += k[b];
  k[c] ^= k[a];
  _, _, k[c] = #ROL_32(k[c], r);
  return k;
}

// not exported; may be useful as spec;
inline fn __quarter_round_ref(reg u32[16] k, inline int a b c d) -> reg u32[16]
{
  k = __line_ref(k, a, b, d, 16);
  k = __line_ref(k, c, d, b, 12);
  k = __line_ref(k, a, b, d, 8);
  k = __line_ref(k, c, d, b, 7);
  return k;
}

// not exported; may be useful as spec;
inline fn __column_round_ref(reg u32[16] k, stack u32 k15) -> reg u32[16], stack u32
{
  stack u32 k14;

  k = __quarter_round_ref(k, 0, 4,  8, 12);
  k = __quarter_round_ref(k, 1, 5,  9, 13);
  k = __quarter_round_ref(k, 2, 6, 10, 14);  k14 = k[14]; k[15] = k15;
  k = __quarter_round_ref(k, 3, 7, 11, 15);  k15 = k[15]; k[14] = k14;

  return k, k15;
}

// not exported; may be useful as spec;
inline fn __diagonal_round_ref(reg u32[16] k, stack u32 k15) -> reg u32[16], stack u32
{
  stack u32 k14;
                                            k14 = k[14]; k[15] = k15;
  k = __quarter_round_ref(k, 0, 5, 10, 15); k15 = k[15]; k[14] = k14;
  k = __quarter_round_ref(k, 1, 6, 11, 12);
  k = __quarter_round_ref(k, 2, 7, 8,  13);
  k = __quarter_round_ref(k, 3, 4, 9,  14);

  return k, k15;
}

// not exported; may be useful as spec;
inline fn __double_round_ref(reg u32[16] k, stack u32 k15) -> reg u32[16], stack u32
{
  k, k15 = __column_round_ref(k, k15);
  k, k15 = __diagonal_round_ref(k, k15);
  return k, k15;
}

// not exported; may be useful as spec;
inline fn __rounds_ref(reg u32[16] k, stack u32 k15) -> reg u32[16], stack u32
{
  stack u32 c;

  c = (CHACHA_ROUNDS/2);
  while
  {
    k, k15 = __double_round_ref(k, k15);
    (_,_,_,_,c) = #DEC_32(c);
  } (c > 0)

  return k, k15;
}
*/

///////////////////////////////////////////////////////////////////////////////


// used; performs two quarter rounds, inlined 'line's;
#nomodmsf inline fn __half_round_inline_ref(
  reg u32[16] k,
  inline int a0 b0 c0 d0
                     a1 b1 c1 d1)
  ->
  reg u32[16]
{

  //k = line(k, a, b, d, 16);
  k[a0] += k[b0];
  k[a1] += k[b1];

  k[d0] ^= k[a0];
  k[d1] ^= k[a1];

  k[d0] = #ROL_32(k[d0], 16);
  k[d1] = #ROL_32(k[d1], 16);

  //k = line(k, c, d, b, 12);
  k[c0] += k[d0];
  k[c1] += k[d1];

  k[b0] ^= k[c0];
  k[b1] ^= k[c1];

  k[b0] = #ROL_32(k[b0], 12);
  k[b1] = #ROL_32(k[b1], 12);

  //k = line(k, a, b, d, 8);
  k[a0] += k[b0];
  k[a1] += k[b1];

  k[d0] ^= k[a0];
  k[d1] ^= k[a1];

  k[d0] = #ROL_32(k[d0], 8);
  k[d1] = #ROL_32(k[d1], 8);

  //k = line(k, c, d, b, 7);
  k[c0] += k[d0];
  k[c1] += k[d1];

  k[b0] ^= k[c0];
  k[b1] ^= k[c1];

  k[b0] = #ROL_32(k[b0], 7);
  k[b1] = #ROL_32(k[b1], 7);

  return k;
}

// used;
#nomodmsf inline fn __double_round_inline_ref(
  reg u32[16] k,
  stack u32 k1 k2 k14 k15
  )
  ->
  reg u32[16],
  stack u32,
  stack u32,
  stack u32,
  stack u32
{
  k[2] = k2; k[14] = k14;

  k = __half_round_inline_ref(k, 0, 4, 8,  12,
                                       2, 6, 10, 14);
  k2 = k[2]; k14 = k[14];
  k[1] = k1; k[15] = k15;

  k = __half_round_inline_ref(k, 1, 5, 9,  13,
                                       3, 7, 11, 15);

  k = __half_round_inline_ref(k, 1, 6, 11, 12,
                                       0, 5, 10, 15);

  k1 = k[1]; k15 = k[15];
  k[2] = k2; k[14] = k14;

  k = __half_round_inline_ref(k, 2, 7, 8, 13,
                                       3, 4, 9, 14);

  k2 = k[2]; k14 = k[14];

  return k, k1, k2, k14, k15;
}



// used;
inline fn __rounds_inline_ref(
  reg u32[16] k,
  stack u32 k14 k15,
  #msf reg u64 ms)
  ->
  reg u32[16],
  stack u32,
  stack u32,
  #msf reg u64
{
  stack u32 k1 k2;
  stack u32 s_c;
  reg u32 c;
  reg bool b;

  k1 = k[1];
  k2 = k[2];

  c = (CHACHA_ROUNDS/2);
  while
  {
    s_c = c;
    k, k1, k2, k14, k15 = __double_round_inline_ref(k, k1, k2, k14, k15);

    //c = s_c;
    c = s_c;
    c = #protect_32(c, ms);

    (_,_,_,_,c) = #DEC_32(c);
    b = (c > 0);
  } (b) { ms = #set_msf(b, ms); }
  ms = #set_msf(!b, ms);

  k[1] = k1;
  k[2] = k2;

  return k, k14, k15, ms;
}



// used;
inline fn __sum_states_ref(
  reg u32[16] k,
  stack u32 k14 k15,
  stack u32[16] st,
  #msf reg u64 ms)
  ->
  reg u32[16],
  stack u32,
  stack u32,
  #msf reg u64
{
  inline int i;
  stack u32 k13;
  reg u32 t;

  for i=0 to 14
  { k[i] += st[i]; }
  k13 = k[13];

  t = k14;
  t += st[14];
  k14 = t;

  t = k15;
  t += st[15];
  k15 = t;

  k[13] = k13;

  return k, k14, k15, ms;
}


inline fn __chacha_xor_ref(reg u64 output plain len nonce key, #msf reg u64 ms) ->  #msf reg u64
{
  stack u64 s_output s_plain s_len;
  stack u32[16] st;
  reg u32[16] k;  // the full state is in k[0..14] and k15;
  stack u32 k14 k15;
  reg bool b;

  s_output = output;
  s_plain = plain;
  s_len = len;

  st = __init_ref(nonce, key);

  while {
    len = s_len;
    len = #protect(len, ms);
    b = (len >= 64);
  } (b)
  {
    ms = #set_msf(b, ms);
    k, k14, k15 = __copy_state_ref(st);
    k, k14, k15, ms = __rounds_inline_ref(k, k14, k15, ms);
    s_output, s_plain, s_len = __sum_states_store_xor_ref(s_output, s_plain, s_len, k, k14, k15, st, ms);
    st = __increment_counter_ref(st);
  }
  ms = #set_msf(!b, ms);

  b = (len > 0);
  if(b)
  {
    ms = #set_msf(b, ms);
    k, k14, k15 = __copy_state_ref(st);
    k, k14, k15, ms = __rounds_inline_ref(k, k14, k15, ms);
    k, k14, k15, ms = __sum_states_ref(k, k14, k15, st, ms);
    ms = __store_xor_last_ref(s_output, s_plain, s_len, k, k14, k15, ms);
  }
  else // CHECKME
  { ms = #set_msf(!b, ms); }

  return ms;
}



inline fn __chacha_ref(reg u64 output len nonce key, #msf reg u64 ms) ->  #msf reg u64
{
  stack u64 s_output s_len;
  stack u32[16] st;
  reg u32[16] k;  // the full state is in k[0..13], k14, and k15;
  stack u32 k14 k15;
  reg bool b;

  s_output = output;
  s_len = len;

  st = __init_ref(nonce, key);

  while {
    len = s_len;
    len = #protect(len, ms);
    b = (len >= 64);
  } (b)
  {
    ms = #set_msf(b, ms);
    k, k14, k15 = __copy_state_ref(st);
    k, k14, k15, ms = __rounds_inline_ref(k, k14, k15, ms);
    s_output, s_len = __sum_states_store_ref(s_output, s_len, k, k14, k15, st, ms);
    st = __increment_counter_ref(st);
  }
  ms = #set_msf(!b, ms);

  b = (len > 0);
  if(b)
  {
    ms = #set_msf(b, ms);
    k, k14, k15 = __copy_state_ref(st);
    k, k14, k15, ms = __rounds_inline_ref(k, k14, k15, ms);
    k, k14, k15, ms = __sum_states_ref(k, k14, k15, st, ms);
    ms = __store_last_ref(s_output, s_len, k, k14, k15, ms);
  }
  else // CHECKME
  { ms = #set_msf(!b, ms); }

  return ms;
}
